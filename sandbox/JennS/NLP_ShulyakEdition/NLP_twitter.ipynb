{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "Stopwords are words that are very common and add little meaning\n",
    "examples: a, of, the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PorterStemmer\n",
    "Words that have the same stem, typically have the same meaning\n",
    "PorterStemmer cuts off the affixes so you just use the stem -> reduces word count (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter_og = pd.read_csv(\"../Twitter_Data_for_NLP.csv\", encoding = \"ISO-8859-1\")\n",
    "data_twitter_og.columns = [\"tweet\", \"sentiment\"]\n",
    "data_twitter_og"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate Empty tweets! Probably pictures or memes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter_og[\"tweet\"] = data_twitter_og[\"tweet\"].astype(\"string\")\n",
    "data_twitter_cleaned = data_twitter_og.loc[pd.notna(data_twitter_og[\"tweet\"]),:].copy()\n",
    "data_twitter = data_twitter_cleaned.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter.reset_index(inplace=True)\n",
    "data_twitter.drop(columns=\"index\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation\n",
    "1. Punctuation percentage\n",
    "2. Text Length\n",
    "3. Captialization percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count punctuation\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    if (len(text) - text.count(\" \")) ==0:   # Need to avoid dividing by 0\n",
    "        return 0\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "# Apply function to make new column\n",
    "data_twitter['punct%'] = data_twitter['tweet'].apply(lambda x: count_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine length of headline and make it a column\n",
    "data_twitter['text_len'] = data_twitter['tweet'].apply(lambda x: len(x) - x.count(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine capitalization percentage\n",
    "def capital_percent(text):\n",
    "    count = sum([1 for char in text if char.isupper()])\n",
    "    if (len(text) - text.count(\" \")) == 0:      # Avoid dividing by 0\n",
    "        return 0\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "# Apply function to make new column\n",
    "data_twitter['capital%'] = data_twitter['tweet'].apply(lambda x: capital_percent(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean up data\n",
    "# Eliminate punctuation\n",
    "# Make everything lowercase\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]  # Use portstemmer\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "Inverse Document Frequency Weighting \n",
    "* Creates a document-term matrix where the cells contain a weighting of how important that word is to the text\n",
    "* How much does a word differentiate a text message from othes? Pulls out important but seldom used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the object and state our parameters. Pass in the function we created to clean the text (clean_text)\n",
    "tfidf_vect_twitter = TfidfVectorizer(analyzer=clean_text) \n",
    "\n",
    "# Fit and transform model \n",
    "X_tfidf_twitter = tfidf_vect_twitter.fit_transform(data_twitter['tweet'])\n",
    "\n",
    "# Create df to see vectorization and concatenated created features\n",
    "X_tfidf_feat_twitter = pd.concat([data_twitter['text_len'], data_twitter['punct%'], data_twitter['capital%'], pd.DataFrame(X_tfidf_twitter.toarray())], axis=1)\n",
    "X_tfidf_feat_twitter.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer\n",
    "* Creates a document term matrix where the entry of each cell will be a count of the number of times that word occurred in that document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the object and state our parameters. Pass in the function we created to clean the text (clean_text)\n",
    "count_vect_twitter = CountVectorizer(analyzer=clean_text)\n",
    "\n",
    "# Fit and Transform model\n",
    "X_count_twitter = count_vect_twitter.fit_transform(data_twitter['tweet'])\n",
    "\n",
    "# Create df to see vectorization and concatenated created features\n",
    "X_count_feat_twitter = pd.concat([data_twitter['text_len'], data_twitter['punct%'], data_twitter['capital%'], pd.DataFrame(X_count_twitter.toarray())], axis=1)\n",
    "\n",
    "X_count_feat_twitter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest on Holdout Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_tfidf_feat_twitter\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_feat_twitter, data_twitter['sentiment'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\n",
    "rf_model = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test)\n",
    "precision, recall, fscore, support = score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round((y_pred==y_test).sum() / len(y_pred),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

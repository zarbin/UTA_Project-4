{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Dependencies\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date, timedelta, datetime\n",
    "import hvplot.pandas\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ticker List, start date\n",
    "tickers = [\"BTC-USD\", #Bitcoin (deleted - \"ETH-USD\", \"LTC-USD\", \"BITW\", ether, litecoin, top 10 crypto index fund)\n",
    "           \"GLD\", \"SLV\", \"CL=F\", #Gold, silver, crude\n",
    "           \"VIXY\", #VIX Short-Term Futures ETF\n",
    "           \"^IXIC\", \"^GSPC\", \"^DJI\", #Nasdaq, s&p, dow\n",
    "           \"META\", \"AMZN\", \"AAPL\", \"NFLX\", \"GOOG\", \"TSLA\", #faangs, telsa\n",
    "           \"JPM\", \"WFC\", \"C\", \"BAC\", #Big US Banks JPM, WFC, C, BAC\n",
    "           \"UUP\", #usd bull fund\n",
    "           \"IEF\" #iShares 7-10 Year Treasury Bond \n",
    "           ]\n",
    "start_date = \"2013-12-31\"\n",
    "filter_date = \"2014-11-06\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Interval lengths\n",
    "long = 50\n",
    "med = 15\n",
    "short = 5\n",
    "start_date = datetime.strptime(start_date, '%Y-%m-%d').date() - timedelta(days = long)\n",
    "filter_date = datetime.strptime(filter_date, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quantiles\n",
    "quantiles = [.01, .05, .1, .5, .9, .95, .99]\n",
    "model_q = .5\n",
    "target_return = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "## Look up ticker and create csv\n",
    "for ticker in tickers:\n",
    "    data = pd.DataFrame(yf.download(ticker, start=start_date))\n",
    "    filename = ticker + '.csv'\n",
    "    data.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-11-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-11-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-11-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-11-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-11-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>2023-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>2023-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>2023-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>2023-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3437</th>\n",
       "      <td>2023-04-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3438 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date\n",
       "0    2013-11-11\n",
       "1    2013-11-12\n",
       "2    2013-11-13\n",
       "3    2013-11-14\n",
       "4    2013-11-15\n",
       "...         ...\n",
       "3433 2023-04-06\n",
       "3434 2023-04-07\n",
       "3435 2023-04-08\n",
       "3436 2023-04-09\n",
       "3437 2023-04-10\n",
       "\n",
       "[3438 rows x 1 columns]"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create comprehensive list of dates\n",
    "#start = datetime.strptime(start_date, '%Y-%m-%d').date() - timedelta(days = long)\n",
    "start = start_date\n",
    "end = date.today() - timedelta(days = 1)\n",
    "delta = timedelta(days=1)\n",
    "\n",
    "dates = []\n",
    "while start <= end:\n",
    "    dates.append(start.isoformat())\n",
    "    start += delta\n",
    "\n",
    "dates = pd.DataFrame(dates)\n",
    "dates.columns =['Date']\n",
    "dates['Date'] = pd.to_datetime(dates['Date'])\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\raminskim\\Anaconda3\\envs\\PythonData2\\lib\\site-packages\\ipykernel_launcher.py:213: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "c:\\Users\\raminskim\\Anaconda3\\envs\\PythonData2\\lib\\site-packages\\ipykernel_launcher.py:213: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "c:\\Users\\raminskim\\Anaconda3\\envs\\PythonData2\\lib\\site-packages\\ipykernel_launcher.py:213: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "c:\\Users\\raminskim\\Anaconda3\\envs\\PythonData2\\lib\\site-packages\\ipykernel_launcher.py:213: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "c:\\Users\\raminskim\\Anaconda3\\envs\\PythonData2\\lib\\site-packages\\ipykernel_launcher.py:213: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "c:\\Users\\raminskim\\Anaconda3\\envs\\PythonData2\\lib\\site-packages\\ipykernel_launcher.py:213: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "c:\\Users\\raminskim\\Anaconda3\\envs\\PythonData2\\lib\\site-packages\\ipykernel_launcher.py:213: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "c:\\Users\\raminskim\\Anaconda3\\envs\\PythonData2\\lib\\site-packages\\ipykernel_launcher.py:213: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "c:\\Users\\raminskim\\Anaconda3\\envs\\PythonData2\\lib\\site-packages\\ipykernel_launcher.py:213: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    }
   ],
   "source": [
    "## Create empty pd df for classification analysis\n",
    "df_class = pd.DataFrame(columns=['Ticker', 'avgvol_last_' + str(long), 'hi_to_lo_last_' + str(long), 'avgvol_last_' + str(med), 'hi_to_lo_last_' + str(med),\n",
    "                                 'avgvol_last_' + str(short), 'hi_to_lo_last_' + str(short), 'avgvol_last_1', 'hi_to_lo_last_1'])\n",
    "\n",
    "## Create empty pd df for quantile comparison\n",
    "df_quant = pd.DataFrame(index=quantiles)\n",
    "\n",
    "## Create empty pd df for logistic regression comparison\n",
    "df_logist = pd.DataFrame(columns=['Ticker', 'percentile_' + str(model_q), 'Accuracy', 'TP_rate', 'TN_rate', 'Pos_Predicted_Value', 'Neg_Predicted_Value'])\n",
    "df_logist_profit = pd.DataFrame(columns=['Ticker', 'target_return', 'Accuracy', 'TP_rate', 'TN_rate', 'Pos_Predicted_Value', 'Neg_Predicted_Value'])\n",
    "\n",
    "## Loop thru asset files and add metrics\n",
    "for ticker in tickers:\n",
    "    ## Upload csv datasets\n",
    "    filename = ticker + '.csv'\n",
    "    temp = pd.read_csv(filename)\n",
    "    \n",
    "    ## Format date\n",
    "    temp['Date'] = pd.to_datetime(temp['Date'])\n",
    "    \n",
    "    ## Merge dfs to capture all dates including weekends and market holidays\n",
    "    df = pd.merge(dates, temp, how = 'left', on='Date')\n",
    "    \n",
    "    ## Fill in weekends and market holidays using last trading day\n",
    "    df['Open'].fillna(method='ffill', inplace=True)\n",
    "    df['High'].fillna(method='ffill', inplace=True)\n",
    "    df['Low'].fillna(method='ffill', inplace=True)\n",
    "    df['Close'].fillna(method='ffill', inplace=True)\n",
    "    df['Adj Close'].fillna(method='ffill', inplace=True)\n",
    "    df['Volume'].fillna(method='ffill', inplace=True)\n",
    "\n",
    "    ## Shift metrics to get yesterday's value\n",
    "    df['hi_shift'] = df['High'].shift(1)\n",
    "    df['lo_shift'] = df['Low'].shift(1)\n",
    "    df['adjclose_shift'] = df['Adj Close'].shift(1)\n",
    "    df['vol_shift'] = df['Volume'].shift(1)\n",
    "    \n",
    "    ## Calculate 'Adj Close' for past and future\n",
    "    df['close_last_long'] = df['Adj Close'].shift(long)\n",
    "    df['close_last_med'] = df['Adj Close'].shift(med)\n",
    "    df['close_last_short'] = df['Adj Close'].shift(short)\n",
    "    df['close_last_1'] = df['Adj Close'].shift(2)\n",
    "    df['close_next_1'] = df['Adj Close']\n",
    "    df['close_next_short'] = df['Adj Close'].shift(-short + 1)\n",
    "    df['close_next_med'] = df['Adj Close'].shift(-med + 1)\n",
    "    df['close_next_long'] = df['Adj Close'].shift(-long + 1)\n",
    "    \n",
    "    ## Calculate short, med, long hi/lo, avg vol, return\n",
    "    ## Past\n",
    "    df['avgvol_last_long'] = df['vol_shift'].rolling(long).sum() / long\n",
    "    df['vol_hi_lo_last_long'] = (df['vol_shift'].rolling(long).max() / df['vol_shift'].rolling(long).min()) - 1\n",
    "    df['hi_to_lo_last_long'] = (df['adjclose_shift'].rolling(long).max() / df['adjclose_shift'].rolling(long).min()) - 1\n",
    "    df['return_last_long'] = (df['adjclose_shift'] / df['close_last_long']) - 1\n",
    "    df['avgvol_last_med'] = df['vol_shift'].rolling(med).sum() / med\n",
    "    df['vol_hi_lo_last_med'] = (df['vol_shift'].rolling(med).max() / df['vol_shift'].rolling(med).min()) - 1\n",
    "    df['hi_to_lo_last_med'] = (df['adjclose_shift'].rolling(med).max() / df['adjclose_shift'].rolling(med).min()) - 1\n",
    "    df['return_last_med'] = (df['adjclose_shift'] / df['close_last_med']) - 1\n",
    "    df['avgvol_last_short'] = df['vol_shift'].rolling(short).sum() / short\n",
    "    df['vol_hi_lo_last_short'] = (df['vol_shift'].rolling(short).max() / df['vol_shift'].rolling(short).min()) - 1\n",
    "    df['hi_to_lo_last_short'] = (df['adjclose_shift'].rolling(short).max() / df['adjclose_shift'].rolling(short).min()) - 1\n",
    "    df['return_last_short'] = (df['adjclose_shift'] / df['close_last_short']) - 1\n",
    "    df['hi_to_lo_last_1'] = (df['hi_shift'] / df['lo_shift']) - 1\n",
    "    df['return_last_1'] = (df['adjclose_shift'] / df['close_last_1']) - 1\n",
    "    ## Future\n",
    "    df['return_next_1'] = (df['close_next_1'] / df['adjclose_shift']) - 1\n",
    "    df['return_next_short'] = (df['close_next_short'] / df['adjclose_shift']) - 1\n",
    "    df['return_next_med'] = (df['close_next_med'] / df['adjclose_shift']) - 1\n",
    "    df['return_next_long'] = (df['close_next_long'] / df['adjclose_shift']) - 1\n",
    "    \n",
    "    ## Filter out Nulls\n",
    "    df = df.dropna()\n",
    "\n",
    "    ## Find last date in dfs\n",
    "    max_date = df['Date'].max()\n",
    "    \n",
    "    ## Drop unused fields\n",
    "    df = df.drop(columns=['hi_shift', 'lo_shift', 'adjclose_shift',\n",
    "                          'close_last_long', 'close_last_med', 'close_last_short', 'close_last_1', 'close_next_1', 'close_next_long', 'close_next_med', 'close_next_short'])\n",
    "    \n",
    "    ## Rename, add ticker, and rearrange columns\n",
    "    ## Rename\n",
    "    df = df.rename(columns={\"vol_shift\": \"avgvol_last_1\",\n",
    "                            \"avgvol_last_long\": \"avgvol_last_\" + str(long), \n",
    "                            \"vol_hi_lo_last_long\": \"vol_hi_lo_last_\" + str(long),\n",
    "                            \"hi_to_lo_last_long\": \"hi_to_lo_last_\" + str(long),\n",
    "                            \"return_last_long\": \"return_last_\" + str(long),\n",
    "                            \"avgvol_last_med\": \"avgvol_last_\" + str(med), \n",
    "                            \"vol_hi_lo_last_med\": \"vol_hi_lo_last_\" + str(med),\n",
    "                            \"hi_to_lo_last_med\": \"hi_to_lo_last_\" + str(med),\n",
    "                            \"return_last_med\": \"return_last_\" + str(med),\n",
    "                            \"avgvol_last_short\": \"avgvol_last_\" + str(short), \n",
    "                            \"vol_hi_lo_last_short\": \"vol_hi_lo_last_\" + str(short),\n",
    "                            \"hi_to_lo_last_short\": \"hi_to_lo_last_\" + str(short),\n",
    "                            \"return_last_short\": \"return_last_\" + str(short),\n",
    "                            \"return_next_long\": \"return_next_\" + str(long),\n",
    "                            \"return_next_med\": \"return_next_\" + str(med),\n",
    "                            \"return_next_short\": \"return_next_\" + str(short)})\n",
    "    ## Add ticker\n",
    "    df['Ticker'] = ticker\n",
    "\n",
    "    ## Rearrange\n",
    "    df = df[['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', \n",
    "             'avgvol_last_' + str(long), 'vol_hi_lo_last_' + str(long), 'hi_to_lo_last_' + str(long), 'return_last_' + str(long),\n",
    "             'avgvol_last_' + str(med), 'vol_hi_lo_last_' + str(med), 'hi_to_lo_last_' + str(med), 'return_last_' + str(med),\n",
    "             'avgvol_last_' + str(short), 'vol_hi_lo_last_' + str(short), 'hi_to_lo_last_' + str(short), 'return_last_' + str(short),\n",
    "             'avgvol_last_1', 'hi_to_lo_last_1', 'return_last_1',\n",
    "             'return_next_1', 'return_next_' + str(short), 'return_next_' + str(med), 'return_next_' + str(long)]]\n",
    "    \n",
    "    ## Filter dates\n",
    "    df = df.loc[df['Date'] >= pd.to_datetime(filter_date)]\n",
    "\n",
    "    ## Write to individual ticker csv\n",
    "    df.to_csv(filename)\n",
    "\n",
    "    ###########################################\n",
    "\n",
    "    ## df_class\n",
    "    ## Filter df for max_date\n",
    "    df_temp = df.loc[df['Date'] == max_date]\n",
    "    df_temp = df_temp[['Ticker', 'avgvol_last_' + str(long), 'vol_hi_lo_last_' + str(long), 'hi_to_lo_last_' + str(long), \n",
    "                                 'avgvol_last_' + str(med), 'vol_hi_lo_last_' + str(med), 'hi_to_lo_last_' + str(med),\n",
    "                                 'avgvol_last_' + str(short), 'vol_hi_lo_last_' + str(short),  'hi_to_lo_last_' + str(short), \n",
    "                                 'avgvol_last_1', 'hi_to_lo_last_1']]\n",
    "\n",
    "    ## Add row to df_class\n",
    "    df_class = df_class.append(df_temp, ignore_index=True)\n",
    "\n",
    "    ###########################################\n",
    "    \n",
    "    ## df_quant\n",
    "    ## Create df for quantiles of each ticker\n",
    "    df_quant_temp = df[['return_next_' + str(long)]]\n",
    "    df_quant_temp = df_quant_temp.quantile(quantiles)\n",
    "    df_quant_temp = df_quant_temp.rename(columns={'return_next_' + str(long): ticker})\n",
    "\n",
    "    # Merge df with df_blank\n",
    "    df_quant = pd.merge(df_quant, df_quant_temp, left_index=True, right_index=True)\n",
    "\n",
    "    ###########################################\n",
    "\n",
    "    ## df_logist\n",
    "    ## Get the quantile to use for binary classification\n",
    "    q_model = df_quant.filter(items=[model_q], axis=0).iloc[0][ticker]\n",
    "\n",
    "    ## Add 'Outcome' field \n",
    "    df['Outcome'] = np.where(df['return_next_' + str(long)] >= q_model, 1, 0)\n",
    "\n",
    "    ## Establish y and X variables\n",
    "    y = df['Outcome']\n",
    "    X = df.drop(columns=['Outcome', 'Date', 'Ticker', \n",
    "                         'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
    "                         'return_next_1', 'return_next_' + str(short), 'return_next_' + str(med)])\n",
    "\n",
    "    ## Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "\n",
    "    ## Create Logistic Regression Model\n",
    "    classifier = LogisticRegression(solver='lbfgs', max_iter=200, random_state=1)\n",
    "\n",
    "    ## Fit model on training data\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    ## Make predictions\n",
    "    predictions = classifier.predict(X_test)\n",
    "    \n",
    "    ## Calculate confusion matrix, accuracy score, TP_rate, TN_rate\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    TP = cm[0][0]\n",
    "    FP = cm[0][1]\n",
    "    FN = cm[1][0]\n",
    "    TN = cm[1][1]\n",
    "    TP_rate = TP / (TP + FN)\n",
    "    TN_rate = TN / (TN + FP)\n",
    "    PPV_rate = TP / (TP + FP)\n",
    "    NPV_rate = TN / (TN + FN)\n",
    "\n",
    "    ## Append df_logist with each asset score\n",
    "    df_logist_temp = {'Ticker':ticker, 'percentile_' + str(model_q): q_model, 'Accuracy':accuracy, 'TP_rate': TP_rate, 'TN_rate': TN_rate, 'Pos_Predicted_Value': PPV_rate, 'Neg_Predicted_Value': NPV_rate}\n",
    "    df_logist = df_logist.append(df_logist_temp, ignore_index=True)\n",
    "\n",
    "    ###########################################\n",
    "    \n",
    "    ## df_logist_prof\n",
    "    ## Add 'Outcome' field \n",
    "    df['Outcome2'] = np.where(df['return_next_' + str(long)] >= target_return, 1, 0)\n",
    "\n",
    "    ## Establish y and X variables\n",
    "    y2 = df['Outcome2']\n",
    "    X2 = df.drop(columns=['Outcome', 'Outcome2', 'Date', 'Ticker', \n",
    "                         'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
    "                         'return_next_1', 'return_next_' + str(short), 'return_next_' + str(med)])\n",
    "\n",
    "    ## Split into train and test\n",
    "    X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, random_state=1, stratify=y2)\n",
    "\n",
    "    ## Create Logistic Regression Model\n",
    "    classifier2 = LogisticRegression(solver='lbfgs', max_iter=200, random_state=1)\n",
    "\n",
    "    ## Fit model on training data\n",
    "    classifier2.fit(X_train2, y_train2)\n",
    "\n",
    "    ## Make predictions\n",
    "    predictions2 = classifier2.predict(X_test2)\n",
    "    \n",
    "    ## Calculate confusion matrix, accuracy score, TP_rate, TN_rate\n",
    "    cm2 = confusion_matrix(y_test2, predictions2)\n",
    "    accuracy2 = accuracy_score(y_test2, predictions2)\n",
    "    TP2 = cm2[0][0]\n",
    "    FP2 = cm2[0][1]\n",
    "    FN2 = cm2[1][0]\n",
    "    TN2 = cm2[1][1]\n",
    "    TP_rate2 = TP2 / (TP2 + FN2)\n",
    "    TN_rate2 = TN2 / (TN2 + FP2)\n",
    "    PPV_rate2 = TP2 / (TP2 + FP2)\n",
    "    NPV_rate2 = TN2 / (TN2 + FN2)\n",
    "\n",
    "    ## Append df_logist with each asset score\n",
    "    df_logist_temp2 = {'Ticker':ticker, 'target_return': target_return, 'Accuracy':accuracy2, 'TP_rate': TP_rate2, 'TN_rate': TN_rate2, 'Pos_Predicted_Value': PPV_rate2, 'Neg_Predicted_Value': NPV_rate2}\n",
    "    df_logist_profit = df_logist_profit.append(df_logist_temp2, ignore_index=True)\n",
    "\n",
    "\n",
    "###########################################\n",
    "\n",
    "## Write to classification csv\n",
    "df_class.to_csv('df_class.csv')\n",
    "\n",
    "## Write to quantile csv\n",
    "df_quant.to_csv('df_quant.csv')\n",
    "\n",
    "## Write to logistic csv\n",
    "df_logist.to_csv('df_logist.csv')\n",
    "\n",
    "## Write to logistic profit csv\n",
    "df_logist_profit.to_csv('df_logist_profit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='10716'>\n",
       "  <div class=\"bk-root\" id=\"eaf2482c-b378-4446-b832-aac5241711cb\" data-root-id=\"10716\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    var docs_json = {\"e4647cc6-9955-44e5-af83-b0632945d3ed\":{\"defs\":[{\"extends\":null,\"module\":null,\"name\":\"ReactiveHTML1\",\"overrides\":[],\"properties\":[]},{\"extends\":null,\"module\":null,\"name\":\"FlexBox1\",\"overrides\":[],\"properties\":[{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_content\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"align_items\"},{\"default\":\"row\",\"kind\":null,\"name\":\"flex_direction\"},{\"default\":\"wrap\",\"kind\":null,\"name\":\"flex_wrap\"},{\"default\":\"flex-start\",\"kind\":null,\"name\":\"justify_content\"}]},{\"extends\":null,\"module\":null,\"name\":\"GridStack1\",\"overrides\":[],\"properties\":[{\"default\":\"warn\",\"kind\":null,\"name\":\"mode\"},{\"default\":null,\"kind\":null,\"name\":\"ncols\"},{\"default\":null,\"kind\":null,\"name\":\"nrows\"},{\"default\":true,\"kind\":null,\"name\":\"allow_resize\"},{\"default\":true,\"kind\":null,\"name\":\"allow_drag\"},{\"default\":[],\"kind\":null,\"name\":\"state\"}]},{\"extends\":null,\"module\":null,\"name\":\"click1\",\"overrides\":[],\"properties\":[{\"default\":\"\",\"kind\":null,\"name\":\"terminal_output\"},{\"default\":\"\",\"kind\":null,\"name\":\"debug_name\"},{\"default\":0,\"kind\":null,\"name\":\"clears\"}]},{\"extends\":null,\"module\":null,\"name\":\"NotificationAreaBase1\",\"overrides\":[],\"properties\":[{\"default\":\"bottom-right\",\"kind\":null,\"name\":\"position\"},{\"default\":0,\"kind\":null,\"name\":\"_clear\"}]},{\"extends\":null,\"module\":null,\"name\":\"NotificationArea1\",\"overrides\":[],\"properties\":[{\"default\":[],\"kind\":null,\"name\":\"notifications\"},{\"default\":\"bottom-right\",\"kind\":null,\"name\":\"position\"},{\"default\":0,\"kind\":null,\"name\":\"_clear\"},{\"default\":[{\"background\":\"#ffc107\",\"icon\":{\"className\":\"fas fa-exclamation-triangle\",\"color\":\"white\",\"tagName\":\"i\"},\"type\":\"warning\"},{\"background\":\"#007bff\",\"icon\":{\"className\":\"fas fa-info-circle\",\"color\":\"white\",\"tagName\":\"i\"},\"type\":\"info\"}],\"kind\":null,\"name\":\"types\"}]},{\"extends\":null,\"module\":null,\"name\":\"Notification\",\"overrides\":[],\"properties\":[{\"default\":null,\"kind\":null,\"name\":\"background\"},{\"default\":3000,\"kind\":null,\"name\":\"duration\"},{\"default\":null,\"kind\":null,\"name\":\"icon\"},{\"default\":\"\",\"kind\":null,\"name\":\"message\"},{\"default\":null,\"kind\":null,\"name\":\"notification_type\"},{\"default\":false,\"kind\":null,\"name\":\"_destroyed\"}]},{\"extends\":null,\"module\":null,\"name\":\"TemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]},{\"extends\":null,\"module\":null,\"name\":\"MaterialTemplateActions1\",\"overrides\":[],\"properties\":[{\"default\":0,\"kind\":null,\"name\":\"open_modal\"},{\"default\":0,\"kind\":null,\"name\":\"close_modal\"}]}],\"roots\":{\"references\":[{\"attributes\":{\"data\":{\"Assigned_Cluster\":[0,0],\"Ticker\":[\"BTC-USD\",\"TSLA\"],\"hi_to_lo_last_50\":{\"__ndarray__\":\"SnT9kr6u8T/ETABQB7gKQA==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]},\"trans_avgvol_last_50\":{\"__ndarray__\":\"MpEMRsfvAkC1uNIyc/bhPw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[2]}},\"selected\":{\"id\":\"10761\"},\"selection_policy\":{\"id\":\"10775\"}},\"id\":\"10760\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"end\":3.76222243278813,\"reset_end\":3.76222243278813,\"reset_start\":-1.3061543907221727,\"start\":-1.3061543907221727,\"tags\":[[[\"hi_to_lo_last_50\",\"hi_to_lo_last_50\",null]]]},\"id\":\"10719\",\"type\":\"Range1d\"},{\"attributes\":{\"source\":{\"id\":\"10781\"}},\"id\":\"10788\",\"type\":\"CDSView\"},{\"attributes\":{\"data\":{\"Assigned_Cluster\":[2,2,2],\"Ticker\":[\"^IXIC\",\"^GSPC\",\"^DJI\"],\"hi_to_lo_last_50\":{\"__ndarray__\":\"eHQSLbtBzb+EUgTdyKvjv2TSFV1XrOu/\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[3]},\"trans_avgvol_last_50\":{\"__ndarray__\":\"BhCAfwTV/D+UXQtUACv7P+IMhpOvP+g/\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[3]}},\"selected\":{\"id\":\"10804\"},\"selection_policy\":{\"id\":\"10822\"}},\"id\":\"10803\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"label\":{\"value\":\"0\"},\"renderers\":[{\"id\":\"10766\"}]},\"id\":\"10779\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"10734\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"10798\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"click_policy\":\"mute\",\"coordinates\":null,\"group\":null,\"items\":[{\"id\":\"10779\"},{\"id\":\"10801\"},{\"id\":\"10825\"},{\"id\":\"10851\"}],\"location\":[0,0],\"title\":\"Assigned_Cluster\"},\"id\":\"10778\",\"type\":\"Legend\"},{\"attributes\":{\"overlay\":{\"id\":\"10746\"}},\"id\":\"10744\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"10758\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"axis_label\":\"trans_avgvol_last_50\",\"coordinates\":null,\"formatter\":{\"id\":\"10755\"},\"group\":null,\"major_label_policy\":{\"id\":\"10756\"},\"ticker\":{\"id\":\"10734\"}},\"id\":\"10733\",\"type\":\"LinearAxis\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"value\":\"#6d904f\"},\"hatch_alpha\":{\"value\":0.2},\"hatch_color\":{\"value\":\"#6d904f\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"#6d904f\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10832\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"10738\",\"type\":\"BasicTicker\"},{\"attributes\":{\"children\":[{\"id\":\"10717\"},{\"id\":\"10724\"},{\"id\":\"10869\"}],\"margin\":[0,0,0,0],\"name\":\"Row11448\",\"tags\":[\"embedded\"]},\"id\":\"10716\",\"type\":\"Row\"},{\"attributes\":{},\"id\":\"10742\",\"type\":\"PanTool\"},{\"attributes\":{\"angle\":{\"value\":0.0},\"fill_alpha\":{\"value\":1.0},\"fill_color\":{\"value\":\"#6d904f\"},\"hatch_alpha\":{\"value\":1.0},\"hatch_color\":{\"value\":\"#6d904f\"},\"hatch_scale\":{\"value\":12.0},\"hatch_weight\":{\"value\":1.0},\"line_alpha\":{\"value\":1.0},\"line_cap\":{\"value\":\"butt\"},\"line_color\":{\"value\":\"#6d904f\"},\"line_dash\":{\"value\":[]},\"line_dash_offset\":{\"value\":0},\"line_join\":{\"value\":\"bevel\"},\"line_width\":{\"value\":1},\"marker\":{\"value\":\"circle\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10852\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"10745\",\"type\":\"ResetTool\"},{\"attributes\":{\"tools\":[{\"id\":\"10720\"},{\"id\":\"10741\"},{\"id\":\"10742\"},{\"id\":\"10743\"},{\"id\":\"10744\"},{\"id\":\"10745\"}]},\"id\":\"10747\",\"type\":\"Toolbar\"},{\"attributes\":{\"coordinates\":null,\"group\":null,\"text\":\"Scatter Plot by Asset\",\"text_color\":\"black\",\"text_font_size\":\"12pt\"},\"id\":\"10725\",\"type\":\"Title\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"value\":\"#e5ae38\"},\"hatch_alpha\":{\"value\":0.2},\"hatch_color\":{\"value\":\"#e5ae38\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"#e5ae38\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10808\",\"type\":\"Scatter\"},{\"attributes\":{\"source\":{\"id\":\"10760\"}},\"id\":\"10767\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"10848\",\"type\":\"UnionRenderers\"},{\"attributes\":{},\"id\":\"10729\",\"type\":\"LinearScale\"},{\"attributes\":{\"label\":{\"value\":\"2\"},\"renderers\":[{\"id\":\"10809\"}]},\"id\":\"10825\",\"type\":\"LegendItem\"},{\"attributes\":{\"data\":{\"Assigned_Cluster\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"Ticker\":[\"GLD\",\"SLV\",\"CL=F\",\"VIXY\",\"META\",\"AMZN\",\"AAPL\",\"NFLX\",\"GOOG\",\"JPM\",\"WFC\",\"C\",\"BAC\",\"UUP\"],\"hi_to_lo_last_50\":{\"__ndarray__\":\"APb4zE5957/3OYMiy1rfv4DY2B5SY+C/AN4goYjCzj9Z0+6BZZD5PyJsWvnQpuE/d7NgttxCpT84CiRdWUGtP3ksAGXNkLU/i9G47yAn5r+ubuCnM07Ov28ZMdgTV9a/xA2w4d/d4b9XoVM+AUjsvw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[14]},\"trans_avgvol_last_50\":{\"__ndarray__\":\"rv1BEJII6L+dS8vCBmvXv/g7k3aAh/6/xX5ZslIX6L/FKCnRDyq7v0Ezy7YKrco/SMWCmpKtyD85DpEAgUrjv+H6D8cv676/m+9CveEG4r/9Bfz+hWjUv5bffBhYf9a/Oy0aqKqEq79Hnvn6P5zuvw==\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[14]}},\"selected\":{\"id\":\"10782\"},\"selection_policy\":{\"id\":\"10798\"}},\"id\":\"10781\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"10731\",\"type\":\"LinearScale\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#30a2da\"},\"hatch_alpha\":{\"value\":0.1},\"hatch_color\":{\"value\":\"#30a2da\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#30a2da\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10764\",\"type\":\"Scatter\"},{\"attributes\":{\"fill_color\":{\"value\":\"#30a2da\"},\"hatch_color\":{\"value\":\"#30a2da\"},\"line_color\":{\"value\":\"#30a2da\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10763\",\"type\":\"Scatter\"},{\"attributes\":{\"angle\":{\"value\":0.0},\"fill_alpha\":{\"value\":1.0},\"fill_color\":{\"value\":\"#e5ae38\"},\"hatch_alpha\":{\"value\":1.0},\"hatch_color\":{\"value\":\"#e5ae38\"},\"hatch_scale\":{\"value\":12.0},\"hatch_weight\":{\"value\":1.0},\"line_alpha\":{\"value\":1.0},\"line_cap\":{\"value\":\"butt\"},\"line_color\":{\"value\":\"#e5ae38\"},\"line_dash\":{\"value\":[]},\"line_dash_offset\":{\"value\":0},\"line_join\":{\"value\":\"bevel\"},\"line_width\":{\"value\":1},\"marker\":{\"value\":\"circle\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10826\",\"type\":\"Scatter\"},{\"attributes\":{\"end\":2.5503004801535507,\"reset_end\":2.5503004801535507,\"reset_start\":-2.091302673533522,\"start\":-2.091302673533522,\"tags\":[[[\"trans_avgvol_last_50\",\"trans_avgvol_last_50\",null]]]},\"id\":\"10718\",\"type\":\"Range1d\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"value\":\"#fc4f30\"},\"hatch_alpha\":{\"value\":0.2},\"hatch_color\":{\"value\":\"#fc4f30\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"#fc4f30\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10786\",\"type\":\"Scatter\"},{\"attributes\":{\"source\":{\"id\":\"10827\"}},\"id\":\"10834\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"10828\",\"type\":\"Selection\"},{\"attributes\":{\"fill_color\":{\"value\":\"#e5ae38\"},\"hatch_color\":{\"value\":\"#e5ae38\"},\"line_color\":{\"value\":\"#e5ae38\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10806\",\"type\":\"Scatter\"},{\"attributes\":{\"label\":{\"value\":\"1\"},\"renderers\":[{\"id\":\"10787\"}]},\"id\":\"10801\",\"type\":\"LegendItem\"},{\"attributes\":{\"fill_color\":{\"value\":\"#fc4f30\"},\"hatch_color\":{\"value\":\"#fc4f30\"},\"line_color\":{\"value\":\"#fc4f30\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10784\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"10759\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"10804\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"10761\",\"type\":\"Selection\"},{\"attributes\":{\"axis\":{\"id\":\"10733\"},\"coordinates\":null,\"grid_line_color\":null,\"group\":null,\"ticker\":null},\"id\":\"10736\",\"type\":\"Grid\"},{\"attributes\":{\"bottom_units\":\"screen\",\"coordinates\":null,\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"group\":null,\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"10746\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"10827\"},\"glyph\":{\"id\":\"10830\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"10832\"},\"nonselection_glyph\":{\"id\":\"10831\"},\"selection_glyph\":{\"id\":\"10852\"},\"view\":{\"id\":\"10834\"}},\"id\":\"10833\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"10760\"},\"glyph\":{\"id\":\"10763\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"10765\"},\"nonselection_glyph\":{\"id\":\"10764\"},\"selection_glyph\":{\"id\":\"10780\"},\"view\":{\"id\":\"10767\"}},\"id\":\"10766\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"angle\":{\"value\":0.0},\"fill_alpha\":{\"value\":1.0},\"fill_color\":{\"value\":\"#30a2da\"},\"hatch_alpha\":{\"value\":1.0},\"hatch_color\":{\"value\":\"#30a2da\"},\"hatch_scale\":{\"value\":12.0},\"hatch_weight\":{\"value\":1.0},\"line_alpha\":{\"value\":1.0},\"line_cap\":{\"value\":\"butt\"},\"line_color\":{\"value\":\"#30a2da\"},\"line_dash\":{\"value\":[]},\"line_dash_offset\":{\"value\":0},\"line_join\":{\"value\":\"bevel\"},\"line_width\":{\"value\":1},\"marker\":{\"value\":\"circle\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10780\",\"type\":\"Scatter\"},{\"attributes\":{\"callback\":null,\"renderers\":[{\"id\":\"10766\"},{\"id\":\"10787\"},{\"id\":\"10809\"},{\"id\":\"10833\"}],\"tags\":[\"hv_created\"],\"tooltips\":[[\"Assigned_Cluster\",\"@{Assigned_Cluster}\"],[\"trans_avgvol_last_50\",\"@{trans_avgvol_last_50}\"],[\"hi_to_lo_last_50\",\"@{hi_to_lo_last_50}\"],[\"Ticker\",\"@{Ticker}\"]]},\"id\":\"10720\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"10822\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"label\":{\"value\":\"3\"},\"renderers\":[{\"id\":\"10833\"}]},\"id\":\"10851\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"10782\",\"type\":\"Selection\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"value\":\"#30a2da\"},\"hatch_alpha\":{\"value\":0.2},\"hatch_color\":{\"value\":\"#30a2da\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"#30a2da\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10765\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"10755\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"HSpacer11453\",\"sizing_mode\":\"stretch_width\"},\"id\":\"10869\",\"type\":\"Spacer\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#6d904f\"},\"hatch_alpha\":{\"value\":0.1},\"hatch_color\":{\"value\":\"#6d904f\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#6d904f\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10831\",\"type\":\"Scatter\"},{\"attributes\":{\"fill_color\":{\"value\":\"#6d904f\"},\"hatch_color\":{\"value\":\"#6d904f\"},\"line_color\":{\"value\":\"#6d904f\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10830\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"10743\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"10775\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"axis_label\":\"hi_to_lo_last_50\",\"coordinates\":null,\"formatter\":{\"id\":\"10758\"},\"group\":null,\"major_label_policy\":{\"id\":\"10759\"},\"ticker\":{\"id\":\"10738\"}},\"id\":\"10737\",\"type\":\"LinearAxis\"},{\"attributes\":{\"axis\":{\"id\":\"10737\"},\"coordinates\":null,\"dimension\":1,\"grid_line_color\":null,\"group\":null,\"ticker\":null},\"id\":\"10740\",\"type\":\"Grid\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"10781\"},\"glyph\":{\"id\":\"10784\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"10786\"},\"nonselection_glyph\":{\"id\":\"10785\"},\"selection_glyph\":{\"id\":\"10802\"},\"view\":{\"id\":\"10788\"}},\"id\":\"10787\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#e5ae38\"},\"hatch_alpha\":{\"value\":0.1},\"hatch_color\":{\"value\":\"#e5ae38\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#e5ae38\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10807\",\"type\":\"Scatter\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"10803\"},\"glyph\":{\"id\":\"10806\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"10808\"},\"nonselection_glyph\":{\"id\":\"10807\"},\"selection_glyph\":{\"id\":\"10826\"},\"view\":{\"id\":\"10810\"}},\"id\":\"10809\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"data\":{\"Assigned_Cluster\":[3],\"Ticker\":[\"IEF\"],\"hi_to_lo_last_50\":{\"__ndarray__\":\"omAVMYdN678=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[1]},\"trans_avgvol_last_50\":{\"__ndarray__\":\"UPO6ZgOL578=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[1]}},\"selected\":{\"id\":\"10828\"},\"selection_policy\":{\"id\":\"10848\"}},\"id\":\"10827\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"below\":[{\"id\":\"10733\"}],\"center\":[{\"id\":\"10736\"},{\"id\":\"10740\"}],\"height\":300,\"left\":[{\"id\":\"10737\"}],\"margin\":[5,5,5,5],\"min_border_bottom\":10,\"min_border_left\":10,\"min_border_right\":10,\"min_border_top\":10,\"renderers\":[{\"id\":\"10766\"},{\"id\":\"10787\"},{\"id\":\"10809\"},{\"id\":\"10833\"}],\"right\":[{\"id\":\"10778\"}],\"sizing_mode\":\"fixed\",\"title\":{\"id\":\"10725\"},\"toolbar\":{\"id\":\"10747\"},\"width\":700,\"x_range\":{\"id\":\"10718\"},\"x_scale\":{\"id\":\"10729\"},\"y_range\":{\"id\":\"10719\"},\"y_scale\":{\"id\":\"10731\"}},\"id\":\"10724\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"margin\":[5,5,5,5],\"name\":\"HSpacer11452\",\"sizing_mode\":\"stretch_width\"},\"id\":\"10717\",\"type\":\"Spacer\"},{\"attributes\":{\"angle\":{\"value\":0.0},\"fill_alpha\":{\"value\":1.0},\"fill_color\":{\"value\":\"#fc4f30\"},\"hatch_alpha\":{\"value\":1.0},\"hatch_color\":{\"value\":\"#fc4f30\"},\"hatch_scale\":{\"value\":12.0},\"hatch_weight\":{\"value\":1.0},\"line_alpha\":{\"value\":1.0},\"line_cap\":{\"value\":\"butt\"},\"line_color\":{\"value\":\"#fc4f30\"},\"line_dash\":{\"value\":[]},\"line_dash_offset\":{\"value\":0},\"line_join\":{\"value\":\"bevel\"},\"line_width\":{\"value\":1},\"marker\":{\"value\":\"circle\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10802\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"10741\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"10756\",\"type\":\"AllLabels\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#fc4f30\"},\"hatch_alpha\":{\"value\":0.1},\"hatch_color\":{\"value\":\"#fc4f30\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#fc4f30\"},\"size\":{\"value\":5.477225575051661},\"x\":{\"field\":\"trans_avgvol_last_50\"},\"y\":{\"field\":\"hi_to_lo_last_50\"}},\"id\":\"10785\",\"type\":\"Scatter\"},{\"attributes\":{\"source\":{\"id\":\"10803\"}},\"id\":\"10810\",\"type\":\"CDSView\"}],\"root_ids\":[\"10716\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.3\"}};\n",
       "    var render_items = [{\"docid\":\"e4647cc6-9955-44e5-af83-b0632945d3ed\",\"root_ids\":[\"10716\"],\"roots\":{\"10716\":\"eaf2482c-b378-4446-b832-aac5241711cb\"}}];\n",
       "    root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && (id_el.children[0].className === 'bk-root')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       ":NdOverlay   [Assigned_Cluster]\n",
       "   :Scatter   [trans_avgvol_last_50]   (hi_to_lo_last_50,Ticker)"
      ]
     },
     "execution_count": 742,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "10716"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## K-means Classification\n",
    "\n",
    "## Create new variables for log of average volume\n",
    "df_class['trans_avgvol_last_1'] = np.log2(df_class['avgvol_last_1'])\n",
    "df_class['trans_avgvol_last_' + str(short)] = np.log2(df_class['avgvol_last_' + str(short)])\n",
    "df_class['trans_avgvol_last_' + str(med)] = np.log2(df_class['avgvol_last_' + str(med)])\n",
    "df_class['trans_avgvol_last_' + str(long)] = np.log2(df_class['avgvol_last_' + str(long)])\n",
    "\n",
    "## Variable list\n",
    "class_cols = ['trans_avgvol_last_' + str(long), 'vol_hi_lo_last_' + str(long), 'hi_to_lo_last_' + str(long), \n",
    "              'trans_avgvol_last_' + str(med), 'vol_hi_lo_last_' + str(med), 'hi_to_lo_last_' + str(med),\n",
    "              'trans_avgvol_last_' + str(short), 'vol_hi_lo_last_' + str(short), 'hi_to_lo_last_' + str(short), \n",
    "              'trans_avgvol_last_1', 'hi_to_lo_last_1']\n",
    "\n",
    "## Scale the data\n",
    "df_class_scaled = StandardScaler().fit_transform(df_class[class_cols])\n",
    "\n",
    "# Create a DataFrame with the scaled data\n",
    "df_class_scaled = pd.DataFrame(df_class_scaled, columns= class_cols)\n",
    "\n",
    "# Copy the tickers names from the original data\n",
    "df_class_scaled[\"Ticker\"] = df_class[\"Ticker\"]\n",
    "\n",
    "# Set the Ticker column as index\n",
    "df_class_scaled = df_class_scaled.set_index(\"Ticker\")\n",
    "\n",
    "# Initialize the K-Means model with n_clusters\n",
    "model = KMeans(n_clusters=4, random_state=88)\n",
    "\n",
    "# Fit the model for the df_stocks_scaled DataFrame\n",
    "model.fit(df_class_scaled)\n",
    "\n",
    "# Predict the model segments (clusters)\n",
    "asset_clusters = model.predict(df_class_scaled)\n",
    "\n",
    "# Create a new column in the DataFrame with the predicted clusters\n",
    "df_class_scaled[\"Assigned_Cluster\"] = asset_clusters\n",
    "\n",
    "# Create a scatter plot with x=\"AnnualVariance:,  y=\"AnnualReturn\"\n",
    "df_class_scaled.hvplot.scatter(\n",
    "    x='trans_avgvol_last_' + str(long),\n",
    "    y='hi_to_lo_last_' + str(long),\n",
    "    by=\"Assigned_Cluster\",\n",
    "    hover_cols = [\"Ticker\"], \n",
    "    title = \"Scatter Plot by Asset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional features data sources\n",
    "# Consumer Sentiment Data\n",
    "# http://www.sca.isr.umich.edu/tables.html\n",
    "\n",
    "# Gallup polls instituional confidence \n",
    "# https://news.gallup.com/poll/1597/confidence-institutions.aspx \n",
    "\n",
    "# CPI Data\n",
    "# https://www.bls.gov/regions/mid-atlantic/data/consumerpriceindexhistorical_us_table.htm\n",
    "\n",
    "# US GDP\n",
    "# https://www.macrotrends.net/countries/USA/united-states/gdp-gross-domestic-product\n",
    "\n",
    "\n",
    "## Unused #############\n",
    "\n",
    "# Pew polls instituional confidence\n",
    "# https://www.pewresearch.org/politics/2022/06/06/public-trust-in-government-1958-2022/\n",
    "\n",
    "# OECD trust in government\n",
    "# https://data.oecd.org/gga/trust-in-government.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Day</th>\n",
       "      <th>Month</th>\n",
       "      <th>Quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-11-11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-11-12</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-11-13</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-11-14</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-11-15</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>2023-04-08</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>2023-04-09</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3437</th>\n",
       "      <td>2023-04-10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3438 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  Day  Month  Quarter\n",
       "0    2013-11-11    0     11      4.0\n",
       "1    2013-11-12    1     11      4.0\n",
       "2    2013-11-13    2     11      4.0\n",
       "3    2013-11-14    3     11      4.0\n",
       "4    2013-11-15    4     11      4.0\n",
       "...         ...  ...    ...      ...\n",
       "3433 2023-04-06    3      4      2.0\n",
       "3434 2023-04-07    4      4      2.0\n",
       "3435 2023-04-08    5      4      2.0\n",
       "3436 2023-04-09    6      4      2.0\n",
       "3437 2023-04-10    0      4      2.0\n",
       "\n",
       "[3438 rows x 4 columns]"
      ]
     },
     "execution_count": 745,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Date / Time\n",
    "dates2 = dates.copy()\n",
    "dates2['Day'] = pd.to_datetime(dates2['Date']).dt.dayofweek\n",
    "dates2['Month'] = pd.to_datetime(dates2['Date']).dt.month\n",
    "dates2.loc[dates2['Month'] >= 10, 'Quarter'] = 4\n",
    "dates2.loc[dates2['Month'] <= 9, 'Quarter'] = 3\n",
    "dates2.loc[dates2['Month'] <= 6, 'Quarter'] = 2\n",
    "dates2.loc[dates2['Month'] <= 3, 'Quarter'] = 1\n",
    "dates2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Consumer Sentiment Data\n",
    "# http://www.sca.isr.umich.edu/tables.html\n",
    "\n",
    "## Load file\n",
    "csi = pd.read_csv('Resources/tbmics.csv')\n",
    "\n",
    "## Convert month to month number\n",
    "d = {'January':1, 'February':2, 'March':3, 'April':4, 'May':5, 'June':6, 'July':7, 'August':8, 'September':9, 'October':10, 'November':11, 'December':12}\n",
    "csi['Month'] = csi['Month'].map(d)\n",
    "\n",
    "## Create date field, using first day of the month\n",
    "csi['Date'] = pd.to_datetime(dict(year=csi['YYYY'], month=csi['Month'], day=1))\n",
    "\n",
    "## Exclude unnecessary columns\n",
    "csi = csi[['Date', 'ICS_ALL']]\n",
    "\n",
    "## Rename columns\n",
    "csi = csi.rename(columns={\"ICS_ALL\": \"CSI\"})\n",
    "\n",
    "## Merge with date list\n",
    "csi = pd.merge(dates, csi, how = 'left', on='Date')\n",
    "\n",
    "## Interpolate in between readings\n",
    "csi['CSI'].interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gallup Polling Data\n",
    "# https://news.gallup.com/poll/1597/confidence-institutions.aspx\n",
    "\n",
    "## File list\n",
    "polls = ['Resources/gallup_banks.xlsx', 'Resources/gallup_bigbiz.xlsx', 'Resources/gallup_police.xlsx', 'Resources/gallup_pres.xlsx']\n",
    "\n",
    "## Date List\n",
    "gallup = dates\n",
    "\n",
    "for poll in polls:\n",
    "    ## Load file and take first 30 rows\n",
    "    gallup_temp = pd.read_excel(poll)\n",
    "    gallup_temp = gallup_temp.iloc[:30]\n",
    "\n",
    "    ## Create date field, using first day of the month\n",
    "    gallup_temp['Date'] = pd.to_datetime(dict(year=gallup_temp['Year'], month=7, day=1))\n",
    "\n",
    "    ## Exclude unnecessary columns\n",
    "    gallup_temp = gallup_temp[['Date', 'Great deal/Quite a lot']]\n",
    "\n",
    "    ## Rename columns\n",
    "    gallup_temp = gallup_temp.rename(columns={\"Great deal/Quite a lot\": poll[10:len(poll)-5]})\n",
    "\n",
    "    ## Merge with date list\n",
    "    gallup_temp = pd.merge(dates, gallup_temp, how = 'left', on='Date')\n",
    "\n",
    "    ## Interpolate in between readings\n",
    "    gallup_temp[poll[10:len(poll)-5]].interpolate(inplace=True)\n",
    "\n",
    "    ## Merge into gallup df\n",
    "    gallup = pd.merge(gallup, gallup_temp, how = 'left', on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CPI Data\n",
    "# https://www.bls.gov/regions/mid-atlantic/data/consumerpriceindexhistorical_us_table.htm\n",
    "\n",
    "## Load file\n",
    "cpi = pd.read_excel('Resources/cpi.xlsx')\n",
    "\n",
    "## Convert month to month number\n",
    "d = {'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, 'Aug':8, 'Sep':9, 'Oct':10, 'Nov':11, 'Dec':12}\n",
    "cpi['Month'] = cpi['Month'].map(d)\n",
    "\n",
    "## Create date field, using first day of the month\n",
    "cpi['Date'] = pd.to_datetime(dict(year=cpi['Year'], month=cpi['Month'], day=1))\n",
    "\n",
    "## Exclude unnecessary columns\n",
    "cpi = cpi[['Date', 'CPI']]\n",
    "\n",
    "## Merge with date list\n",
    "cpi = pd.merge(dates, cpi, how = 'left', on='Date')\n",
    "\n",
    "## Interpolate in between readings\n",
    "cpi['CPI'].interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>GDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-11-11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-11-12</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-11-13</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-11-14</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-11-15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3433</th>\n",
       "      <td>2023-04-06</td>\n",
       "      <td>5.9455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>2023-04-07</td>\n",
       "      <td>5.9455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3435</th>\n",
       "      <td>2023-04-08</td>\n",
       "      <td>5.9455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3436</th>\n",
       "      <td>2023-04-09</td>\n",
       "      <td>5.9455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3437</th>\n",
       "      <td>2023-04-10</td>\n",
       "      <td>5.9455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3438 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date     GDP\n",
       "0    2013-11-11     NaN\n",
       "1    2013-11-12     NaN\n",
       "2    2013-11-13     NaN\n",
       "3    2013-11-14     NaN\n",
       "4    2013-11-15     NaN\n",
       "...         ...     ...\n",
       "3433 2023-04-06  5.9455\n",
       "3434 2023-04-07  5.9455\n",
       "3435 2023-04-08  5.9455\n",
       "3436 2023-04-09  5.9455\n",
       "3437 2023-04-10  5.9455\n",
       "\n",
       "[3438 rows x 2 columns]"
      ]
     },
     "execution_count": 749,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## US GDP\n",
    "# https://www.macrotrends.net/countries/USA/united-states/gdp-gross-domestic-product\n",
    "\n",
    "## Load file\n",
    "gdp = pd.read_csv('Resources/united-states-gdp-gross-domestic-product.csv')\n",
    "\n",
    "## Convert Date to proper date format\n",
    "gdp['date'] = pd.to_datetime(gdp['date'])\n",
    "\n",
    "## Exclude unnecessary columns\n",
    "gdp = gdp[['date', ' Annual % Change']]\n",
    "\n",
    "## Rename columns\n",
    "gdp = gdp.rename(columns={\"date\": 'Date',\n",
    "                          ' Annual % Change': 'GDP'})\n",
    "\n",
    "## Merge with date list\n",
    "gdp = pd.merge(dates, gdp, how = 'left', on='Date')\n",
    "\n",
    "## Interpolate in between readings\n",
    "gdp['GDP'].interpolate(inplace=True)\n",
    "\n",
    "gdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>vader_avg_last_50</th>\n",
       "      <th>vader_avg_last_15</th>\n",
       "      <th>vader_avg_last_5</th>\n",
       "      <th>vader_avg_last_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.034722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.065192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1834</th>\n",
       "      <td>2023-03-05</td>\n",
       "      <td>0.068715</td>\n",
       "      <td>0.082024</td>\n",
       "      <td>0.098283</td>\n",
       "      <td>0.058089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>2023-03-06</td>\n",
       "      <td>0.068340</td>\n",
       "      <td>0.085956</td>\n",
       "      <td>0.094568</td>\n",
       "      <td>0.093306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>2023-03-07</td>\n",
       "      <td>0.067922</td>\n",
       "      <td>0.084390</td>\n",
       "      <td>0.080840</td>\n",
       "      <td>0.061426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1837</th>\n",
       "      <td>2023-03-08</td>\n",
       "      <td>0.067206</td>\n",
       "      <td>0.081315</td>\n",
       "      <td>0.062457</td>\n",
       "      <td>0.049406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838</th>\n",
       "      <td>2023-03-09</td>\n",
       "      <td>0.065265</td>\n",
       "      <td>0.073648</td>\n",
       "      <td>0.037549</td>\n",
       "      <td>-0.074480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1839 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date  vader_avg_last_50  vader_avg_last_15  vader_avg_last_5  \\\n",
       "0    2018-02-25                NaN                NaN               NaN   \n",
       "1    2018-02-26                NaN                NaN               NaN   \n",
       "2    2018-02-27                NaN                NaN               NaN   \n",
       "3    2018-02-28                NaN                NaN               NaN   \n",
       "4    2018-03-01                NaN                NaN               NaN   \n",
       "...         ...                ...                ...               ...   \n",
       "1834 2023-03-05           0.068715           0.082024          0.098283   \n",
       "1835 2023-03-06           0.068340           0.085956          0.094568   \n",
       "1836 2023-03-07           0.067922           0.084390          0.080840   \n",
       "1837 2023-03-08           0.067206           0.081315          0.062457   \n",
       "1838 2023-03-09           0.065265           0.073648          0.037549   \n",
       "\n",
       "      vader_avg_last_1  \n",
       "0                  NaN  \n",
       "1             0.010650  \n",
       "2             0.055795  \n",
       "3            -0.034722  \n",
       "4             0.065192  \n",
       "...                ...  \n",
       "1834          0.058089  \n",
       "1835          0.093306  \n",
       "1836          0.061426  \n",
       "1837          0.049406  \n",
       "1838         -0.074480  \n",
       "\n",
       "[1839 rows x 5 columns]"
      ]
     },
     "execution_count": 750,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "## Load file\n",
    "vader = pd.read_csv('../JennS/Sentiments/all_crypto_sentiments.csv')\n",
    "\n",
    "## Convert Date to proper date format\n",
    "vader['Date'] = pd.to_datetime(vader['begins_at'])\n",
    "\n",
    "## Rename columns\n",
    "vader['vader_shift'] = vader['vader_prediction'].shift(1)\n",
    "vader['sentiment_shift'] = vader['sentiment'].shift(1)\n",
    "\n",
    "## Calculate rolling values\n",
    "vader['vader_avg_last_long'] = vader['vader_shift'].rolling(long).sum() / long\n",
    "vader['vader_avg_last_med'] = vader['vader_shift'].rolling(med).sum() / med\n",
    "vader['vader_avg_last_short'] = vader['vader_shift'].rolling(short).sum() / short\n",
    "\n",
    "## Rename columns\n",
    "vader = vader.rename(columns={'vader_shift': 'vader_avg_last_1',\n",
    "                              'vader_avg_last_long': 'vader_avg_last_' + str(long),\n",
    "                              'vader_avg_last_med': 'vader_avg_last_' + str(med),\n",
    "                              'vader_avg_last_short': 'vader_avg_last_' + str(short)})\n",
    "\n",
    "## Exclude unnecessary columns\n",
    "vader = vader[['Date', 'vader_avg_last_' + str(long), 'vader_avg_last_' + str(med), 'vader_avg_last_' + str(short), 'vader_avg_last_1']]\n",
    "\n",
    "vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional data features df (merge all)\n",
    "df_addl = pd.merge(dates, dates2, how='left', on='Date')\n",
    "df_addl = pd.merge(df_addl, csi, how = 'left', on='Date')\n",
    "df_addl = pd.merge(df_addl, gallup, how='left', on='Date')\n",
    "df_addl = pd.merge(df_addl, cpi, how='left', on='Date')\n",
    "df_addl = pd.merge(df_addl, gdp, how='left', on='Date')\n",
    "df_addl = pd.merge(df_addl, vader, how='left', on='Date')\n",
    "\n",
    "## Filter datesx\n",
    "df_addl = df_addl.loc[df_addl['Date'] >= pd.to_datetime(filter_date)]\n",
    "\n",
    "## Write to additional csv\n",
    "df_addl.to_csv('df_addl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### SCRATCH ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[310,  69],\n",
       "       [307,  72]], dtype=int64)"
      ]
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Logistic regression\n",
    "df_logist = pd.DataFrame(columns=['Ticker', 'Accuracy'])\n",
    "\n",
    "## Get the quantile to use for binary classification\n",
    "q_model = df_quant.filter(items=[model_q], axis=0).iloc[0][ticker]\n",
    "\n",
    "## Add 'Outcome' field \n",
    "df['Outcome'] = np.where(df['return_next_' + str(long)] >= q_model, 1, 0)\n",
    "\n",
    "## Establish y and X variables\n",
    "y = df['Outcome']\n",
    "X = df.drop(columns=['Outcome', 'Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n",
    "                     'return_next_1', 'return_next_' + str(short), 'return_next_' + str(med)])\n",
    "\n",
    "## Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "\n",
    "## Create Logistic Regression Model\n",
    "classifier = LogisticRegression(solver='lbfgs',\n",
    "                                max_iter=200,\n",
    "                                random_state=1)\n",
    "\n",
    "## Fit model on training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "## Make predictions\n",
    "predictions = classifier.predict(X_test)\n",
    "results = pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test}).reset_index(drop=True)\n",
    "\n",
    "\n",
    "## Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "df_logist_temp = {'Ticker':ticker, 'Accuracy':accuracy}\n",
    "df_logist = df_logist.append(df_logist_temp, ignore_index=True)\n",
    "\n",
    "## Confusion Matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bf4a4e7f6e2b74fd74c37a981ed6c6d83ce25f20bb6a721d0924f01fdba488e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
